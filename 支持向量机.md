# 优化目标
根据之前学过的逻辑回归,

![logistic](image/2021-06-29-20-31-02.png)

我们希望无论在训练集中,交叉验证集中,或是测试集中,如果有一个$y=1$的样本,那么$h_\theta(x)$应该趋于 1,$\theta^Tx$应当远大于0,而根据代价函数
$$
J=-y\log\frac{1}{1+e^{-\theta^Tx}}-(1-y)\log(1-\frac{1}{1+e^{-\theta^Tx}})
$$

$y=1$时,只有第一项起作用,而
$$
-\log\frac{1}{1+e^{-\theta^Tx}}
$$
随着$z=\theta^Tx$的增大而减小
![](image/2021-06-29-20-42-45.png)

这也就解释了，为什么逻辑回归在观察到正样本$y=1$时，试图将$\theta^Tx$设置得非常大.$y=0$时也是同理.

现在开始建立支持向量机.

![cost1](image/2021-06-29-20-44-56.png)

新的代价函数将会水平的从这里到右边(图外)，然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要.

支持向量机，带来计算上的优势.

$y=0$时的情况如下图.

![cost0](image/2021-06-29-20-45-53.png)

将第一个函数命名为$cost_1(z)$,第二个函数命名为$cost_0(z)$.

在逻辑回归中,我们需要找到时代价函数最小的$\theta$,即
$$
\displaystyle \underset{\theta}{min}\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-\log h_\theta(x^{(i)}))+(1-y^{(i)})(-\log(1-h_\theta(x^{(i)})))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$

而在支持向量机中,将其改写为
$$
\displaystyle \underset{\theta}{min}C\frac{1}{m}[\sum_{i=1}^my^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2
$$

我们去掉了式中的$m$,这是人们的书写习惯,当然因为$m$是常量,对最终的结果没有影响.

在支持向量机中,我们不将式子写成$A+\lambda B$的形式,而是写成$C\times A+B$的形式,当然也可以将$C$看做$\frac{1}{\lambda}$

最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数$\theta$时，支持向量机所做的是它来直接预测$y$的值等于 1，还是等于 0。因此，这个假设函数会预测 1。当$\theta^Tx$大于或者等于 0 时，或者等于 0 时，所以学习参数$\theta$就是支持向量机假设函数的形式。那么，这就是支持向量机数学上的定义。

# 大边界的直观理解

人们有时将支持向量机看作是大间距分类器。

![大间距](image/2021-06-29-21-32-47.png)

